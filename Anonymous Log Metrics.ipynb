{"cells": [{"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='537e4f12-aeaa-4390-940a-8a2c406af632', project_access_token='p-9fcf5c343afe96973ef0268a085d5ff38e3cd0b9')\npc = project.project_context", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Watson Assistant Anonymous Log Metrics"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "!curl -O https://raw.githubusercontent.com/cognitive-catalyst/WA-Testing-Tool/master/log_analytics/getAllLogs.py\n!curl -O https://raw.githubusercontent.com/cognitive-catalyst/WA-Testing-Tool/master/log_analytics/extractConversations.py\n\n%load_ext autoreload\n%autoreload 2\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n!pip install ibm-watson\n\nimport json\nimport pandas as pd\nimport getAllLogs\nimport extractConversations", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Custom functions to re-use code throughout notebook\ndef turn_dict_to_df(df,col_names):\n    df = pd.DataFrame.from_dict(df)\n    df.reset_index(level=0, inplace=True)\n    df.columns = col_names\n    return df", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 1. Configuration and log collection <a class=\"anchor\" id=\"config\"></a>\nThis section will configure your DB2 connection, log query parameters, and will extract the logs from your Watson Assistant instance.\n\n> **Action Required:** Update each of the variables marked with 'XXXXXXXX'.  The comments in the cells guide you in the configuration."}, {"metadata": {}, "cell_type": "code", "source": "# Define the customer name. This prefix will be used for saving CSV & JSON files.\ncustName = 'csmbot'\n\n# Set the start date for the log fetch. If you are using the DB2 connection in Section 1.1, this will be defined automatically.\nlog_fetch_start = '2020-05-15'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Retrieve logs from the Watson Assistant instance\nThis section will retrieve the user logs from the Assistant `/logs` API.\n\n> **Action Required:** Update the fields below marked 'XXXXXXXX' based on the credentials of your Assistant. \nSolutions using an Assistant layer (v2 API) should set `workspace_id=None` and provide `assistant_id`. Otherwise, define workspace and comment out assistant_id.\n\n\n"}, {"metadata": {}, "cell_type": "code", "source": "# Extract logs from your assistant. Complete this information.\niam_apikey = 'myapikey' \nurl = \"https://gateway.watsonplatform.net/assistant/api\"# Set the URL to the region, e.g. https://api.us-east.assistant.watson.cloud.ibm.com\nassistant_id = 'myassistantid'\nworkspace_id = None", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# Extract logs from your assistant. Complete this information.\niam_apikey = 'kutJc0CZM6SoaWvkpzyc5eMZfkbApBXiYJBZRBeDLeR5' \nurl = \"https://gateway.watsonplatform.net/assistant/api\"# Set the URL to the region, e.g. https://api.us-east.assistant.watson.cloud.ibm.com\nassistant_id = 'd72e5428-e42b-40dd-98f5-b1e7f5f87f45'\nworkspace_id = None", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# If not using assistant_id, comment out the 2nd line below. \nlog_filter=\"language::en,response_timestamp>=\" + \"2020-01-01\" \\\n+\",request.context.system.assistant_id::\" + assistant_id\n\n#Change the number of logs retrieved, default settings will return 100,000 logs (200 pages of 500)\npage_size_limit=500\npage_num_limit=200\n\nversion=\"2020-09-24\" # Watson Assistant API version\n\nrawLogsJson = getAllLogs.getLogs(iam_apikey, url, workspace_id, log_filter, page_size_limit, page_num_limit, version)\nrawLogsPath= custName + \"_logs.json\"\n\n# getAllLogs.writeLogs(rawLogsJson, rawLogsPath) # Saves the logs locally\nproject.save_data(file_name = rawLogsPath,data = json.dumps(rawLogsJson),overwrite=True); # Saves the logs in Studio/COS\nprint('\\nSaved log data to {}'.format(rawLogsPath))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.3 Load logs from JSON file (Defunct)\nIf you have previously saved the JSON file, you can uncomment this section to load the logs. Otherwise, comment this section out and continue."}, {"metadata": {}, "cell_type": "code", "source": "# #If you have previously stored your logs on the file system, you can reload them here by uncommenting these lines\n# rawLogsPath= custName+\"_logs.json\"\n# rawLogsJson = extractConversations.readLogs(rawLogsPath)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.4 Format logs\nNow that the logs have been retrieved, this section will transform the data out of JSON format and into a Pandas dataframe. \n\n> **Optional:** If you wish to add any custom fields (such as a context variable), add it the first line `customFieldNames` below. Otherwise, run this cell as-is."}, {"metadata": {}, "cell_type": "code", "source": "# Optionally provide a comma-separated list of custom fields you want to extract, in addition to the default fields\ncustomFieldNames = ''\n\n# Unique conversation identifier across all records. This is default. For a multi-skill assistant you will need to provide your own key.\nprimaryLogKey = \"response.context.conversation_id\"\nconversationKey='conversation_id' # Name of the correlating key as it appears in the data frame columns (remove 'response.context.')\n\n# These custom fields are added to the list. They are used for extracting metrics in the notebook. Do not change these.\ncustomFieldNames = customFieldNames + \",response.context.vgwSIPFromURI,response.context.vgwSessionID,request.context.vgwSMSFailureReason,\\\nrequest.context.vgwSMSUserPhoneNumber,response.output.vgwAction.parameters.transferTarget,response.context.language,\\\nresponse.context.metadata.user_id,response.output.generic\"\n\nallLogsDF = extractConversations.extractConversationData(rawLogsJson, primaryLogKey, customFieldNames)\nconversationsGroup = allLogsDF.groupby(conversationKey,as_index=False)\n\n# Splits the response_timestamp into month, day, and year fields that can be used for easier data filtering/visualizations \nallLogsDF[\"full_date\"] = pd.to_datetime(allLogsDF[\"response_timestamp\"])\nallLogsDF['month'] = allLogsDF['full_date'].dt.month\nallLogsDF['day'] = allLogsDF['full_date'].dt.day\nallLogsDF['year'] = allLogsDF['full_date'].dt.year\n\nprint(\"Total log events:\",len(allLogsDF))\nallLogsDF.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Print the column names\n# allLogsDF.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.1 Core Metrics <a class=\"anchor\" id=\"core-metrics\"></a>\nThese metrics apply to all Watson Assistant solutions. For voice solutions, additional metrics are in the next section.\n* [2.1.2 Coverage Metric](#coverage-metric)\n* [2.1.3 Search Skill Responses](#search-skill)\n* [2.1.4 Escalation Requests](#escalation-metric)\n* [2.1.5 Active Users](#active-users)\n* [2.1.6 Top Intents & Average Confidence Scores](#top-intents-scores)\n* [2.1.7 Top Entities](#top-entities)"}, {"metadata": {}, "cell_type": "code", "source": "# dict{} that we will send to CSV for use in Watson Studio Cognos Dashboard\nmetrics_dict = {}\n\n# These should match the count in the Watson Assistant Analytics tooling.\ntotalConvs = len(allLogsDF[conversationKey].unique())\nprint(\"Total messages:     \", len(allLogsDF))\nprint(\"Total conversations:\", totalConvs)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.2 Coverage Metric <a class=\"anchor\" id=\"coverage-metric\"></a>\nCoverage is the measurement of the portion of total user messages that your assistant is attempting to respond to. For example, any messages that respond with \"Sorry I didn't understand\" from the anything_else node is considered uncovered.\n\n> **Action Required:** Define the node ids in `anything_else_nodes` list that represent any responses for uncovered messages. This can be found by exporting the Skill from the Assistant tooling, and searching the JSON for the relevant `dialog_node`. "}, {"metadata": {}, "cell_type": "code", "source": "# Define the node_id for anything_else and other uncovered nodes\nanything_else_nodes = ['Anything else'] \n\n# coveredDF = allLogsDF\nallLogsDF.rename(columns={'input.text': 'input_text'}, inplace=True)\ncoverage = []\n\nfor row in allLogsDF.itertuples():\n    appended = False \n    nodes = row.nodes_visited\n    for node in nodes:\n        if node in anything_else_nodes:\n            coverage.append('uncovered') # Mark as uncovered if message hit node in anything_else_nodes list\n            appended = True\n            break\n    if (row.input_text == '' or row.input_text == 'vgwHangUp' or row.input_text == 'vgwPostResponseTimeout') and not appended:\n        coverage.append('system_message') # Mark greetings and voicegateway actions as system_messages\n        appended = True\n    if not appended:\n        coverage.append('covered') # else, everything else is covered\n\nallLogsDF['coverage'] = coverage\nallLogsDF.rename(columns={'input_text': 'input.text'}, inplace=True)\ncoveredDF = allLogsDF[allLogsDF['coverage'] == 'covered']\nuncoveredDF = allLogsDF[allLogsDF['coverage'] == 'uncovered']\n\nprint('Covered messages:   ', len(coveredDF))\nprint('Uncovered messages: ', len(allLogsDF[allLogsDF['coverage'] == 'uncovered']))\nprint('System messages:    ', len(allLogsDF[allLogsDF['coverage'] == 'system_message']))\nprint('\\nCoverage metric:    ','{:.0%}'.format(len(coveredDF) / filteredMessages))\n\n# coveredMsgs[['input_text','output.text','coverage']].tail(10)\n\nmetrics_dict['coverage'] = [len(coveredDF) / filteredMessages] # Put into metrics dict", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#uncoveredDF[['input.text','output.text']].head(10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.2 Search Skill Responses <a class=\"anchor\" id=\"search-skill\"></a>\nWatson Assistant has multiple response types including `text`, `option`, `image`, `pause`, or `search skill`. Each of these types are marked within `output.generic.response_type` inside the log data. This cell will calculate the number of Search Skill responses."}, {"metadata": {}, "cell_type": "code", "source": "# Run this cell\nresponse_type = []\n\nfor row in allLogsDF['output.generic']:\n    search_skill = False\n    for response in row: # each output can have multiple responses\n        if response['response_type'] == 'search_skill':\n            response_type.append('search_skill')\n            search_skill = True\n            break\n                \n    if not search_skill: # if the response was not a search skill, append other to the list\n        response_type.append('other')\n        \nallLogsDF['response_type'] = response_type # Add in response_type column to allLogsDF\nsearchSkillDF = allLogsDF[allLogsDF['response_type'] == 'search_skill'] # Set new DF \nprint('Total Search Skill responses:',len(searchSkillDF))\nprint('Percentage of total messages: {:.0%}'.format(len(searchSkillDF) / len(allLogsDF) ))\n\nsearchSkillDF[['input.text','response_type']].head().reset_index(drop=True) # Print the list of user inputs that caused search skill", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Saves to CSV\nproject.save_data(file_name = custName + \"_search-skill-inputs.csv\",data = searchSkillDF.to_csv(index=False),overwrite=True); # This saves in COS. Comment out if running locally", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.3 Escalation Requests <a class=\"anchor\" id=\"escalation-metric\"></a>\n\nEscalation refers to any time a user is prompted to contact a live person (e.g. 1-800 number). If the assistant has an integration with a live handoff service (e.g. ZenDesk), this is considered escalation. For Voice Interaction solutions, we calculate `call containment` in the next section by counting the number of actual call transfers in the logs.\n\n> **Action Required:** Define the node id in `escalation_node` for a node that represents any responses to an escalation request (e.g. `#General-Agent-Escalation`). This can be found by exporting the Skill from the Assistant tooling, and searching the JSON for the relevant dialog_node.\n "}, {"metadata": {}, "cell_type": "code", "source": "# Define the escalation node\nescalation_node = \"XXXXXXXX\" \nnode_visits_escalated = allLogsDF[[escalation_node in x for x in allLogsDF['nodes_visited']]]\n\nescalationMetric = len(node_visits_escalated)/filteredMessages\nmetrics_dict['escalation'] = [escalationMetric] # Put into metrics dict\nprint(\"Total visits to escalation node:\",len(node_visits_escalated))\nprint(\"Percent of total messages escalated:\",'{:.0%}'.format(escalationMetric))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.4 Active Users <a class=\"anchor\" id=\"active-users\"></a>\nHow many unique users used the assistant?"}, {"metadata": {}, "cell_type": "code", "source": "uniqueUsers = allLogsDF[\"metadata.user_id\"].nunique()\nmetrics_dict['uniqueUsers'] = [uniqueUsers] # inserts into metrics dict\nprint('Total unique users: {}'.format(uniqueUsers))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.5 Top Intents & Average Confidence Scores <a class=\"anchor\" id=\"top-intents-scores\"></a>"}, {"metadata": {}, "cell_type": "code", "source": "# Using pandas aggregators to count how often each intent is selected and its average confidence\nintentsDF = filteredLogsDF.groupby('intent',as_index=False).agg({\n   'input.text': ['count'], \n   'intent_confidence': ['mean']\n})\n\nintentsDF.columns=[\"intent\",\"count\",\"confidence\"] #Flatten the column headers for ease of use\n\nintentsDF = intentsDF[intentsDF['intent'] !=''] # Remove blanks, usually VGW tags + greetings\nintentsDF = intentsDF.sort_values('count',ascending=False)\nintentsDF = intentsDF.reset_index(drop=True)\nintentsDF.head(5) # If you want specific number shown, edit inside head(). If you want to show all, remove head() ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.6 Top Entities (Defunct) <a class=\"anchor\" id=\"top-entities\"></a>"}, {"metadata": {}, "cell_type": "code", "source": "entityDF = allLogsDF[allLogsDF[\"entities\"] != \"\"]\n#intentsDF = intentsDF[intentsDF['intent'] !=''] # Remove blanks, usually VGW tags + greetings\nentityDF[\"entities\"].iloc[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 3. Export Logs  <a class=\"anchor\" id=\"export-logs\"></a>\nThe transformed log data inside the Pandas dataframe will be saved to CSV files and DB2 on Cloud database. These logs can be used for further data exploration and for creating visualizations in Cognos Dashboard in Watson Studio.\n\n* [3.1 Saving CSV files to Cloud Object Storage](#export-to-csv)  CSV files will be saved to the project's Data Assets and Cloud Object Storage.\n* [3.2 Loading into DB2 on Cloud database](#export-to-db2) The data will be saved to a table on your DB2 instance. \n\n## 3.1 Saving CSV files to Cloud Object Storage <a class=\"anchor\" id=\"export-to-csv\"></a>\nThe data will be saved into a CSV file in Cloud Object Storage, accessible via your project's assets folder in Watson Studio. There will be three distinct CSV files saved:\n* `_logs.csv` will contain all of the data within the allLogs dataframe\n* `_KeyMetrics.csv` will contain the calculated metrics such as coverage, escalation, containment rate, etc.\n* `_uncovered_msgs.csv` will contain the selection of uncovered messages. This file can be used for making improvements to intent training and dialog responses.\n\n\n### 3.1.1 Save all logs to CSV"}, {"metadata": {}, "cell_type": "code", "source": "# allLogsDF.to_csv(custName+'_logs.csv',index=False) # This saves if running notebook locally. Comment out for Studio. \nprint('Saving all logs to {}'.format(custName+ \"_logs.csv\"))\nproject.save_data(file_name = custName + \"_logs.csv\",data = allLogsDF.to_csv(index=False),overwrite=True); # This saves in COS. Comment out if running locally", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1.2 Save KPIs to CSV"}, {"metadata": {}, "cell_type": "code", "source": "metricsDF = pd.DataFrame(metrics_dict)\n# metricsDF.to_csv(custName + \"_KeyMetrics.csv\",index=False) # This saves if running notebook locally. Comment out for Studio. \nproject.save_data(file_name = custName + \"_KeyMetrics.csv\",data = metricsDF.to_csv(index=False),overwrite=True); # This saves in COS. Comment out if running locally\nprint('Saving key metrics to {}'.format(custName+ \"_KeyMetrics.csv\"))\nmetricsDF", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1.3 Save uncovered messages to CSV\nImprove Coverage by analyzing these uncovered messages. This might require adding training data to Intents or customizing STT models."}, {"metadata": {}, "cell_type": "code", "source": "print('\\nSaved', len(uncoveredDF), 'messages to:', custName + \"_uncovered_msgs.csv\")\n# uncoveredDF.to_csv(custName + \"_uncovered_msgs.csv\",index=False, header=['Utterance','Response','Intent','Confidence'])\n\nproject.save_data(file_name = custName + \"_uncovered_msgs.csv\",data = uncoveredDF.to_csv(index=False),overwrite=True); # This saves in COS. Comment out if running locally", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### End of Notebook v2.1 (last modified on 7-2-20)"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}